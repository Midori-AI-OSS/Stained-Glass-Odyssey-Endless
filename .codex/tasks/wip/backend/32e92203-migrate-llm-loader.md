# Migrate LLM Loader to Agent Framework

## Task ID
`32e92203-migrate-llm-loader`

## Priority
High

## Status
PENDING REVIEW - Audit Completed (2025-12-20)

> **ðŸ“‹ TASK MASTER UPDATE (2025-12-20)**  
> Comprehensive execution plan and quick reference created:
> - **Full Plan**: [2792ed45-llm-loader-migration-execution-plan.md](./2792ed45-llm-loader-migration-execution-plan.md)
> - **Quick Ref**: [dba02b8a-llm-loader-quick-reference.md](./dba02b8a-llm-loader-quick-reference.md)
>
> **Key Findings**:
> - âœ… `agent_loader.py` is already complete and functional
> - ðŸ”§ 14 call sites need migration (6 production, 8 test files)
> - â±ï¸ Estimated effort: 9-13 hours in 8 phases
> - âš ï¸ This is a BREAKING CHANGE migration (intentional)
>
> **Prerequisites**: Task 12af34e9-update-dependencies.md must be verified complete
>
> **Ready for Coder Assignment**: See execution plan for detailed phase breakdown

---

## ðŸ” AUDITOR REVIEW (2025-12-20)

### Executive Summary
**Migration Status**: ~90% COMPLETE - Core functionality working but requires fixes before validation

**Overall Assessment**: âš ï¸ CONDITIONAL PASS with required fixes

The LLM loader migration to the Agent Framework has been successfully implemented for the core functionality. All breaking changes were properly applied, production code migrated correctly, and the new `agent_loader.py` is well-designed with good security practices. However, there are **3 CRITICAL issues** that must be resolved before moving to validation:

1. **CRITICAL**: `test_llm_loader.py` is EMPTY (0 bytes) - this file must either be deleted or properly populated
2. **HIGH**: README.md still references deprecated `ModelName` enum (line 97)
3. **HIGH**: Several test files have import/structure issues causing failures

### Detailed Audit Report

#### âœ… BREAKING CHANGES VERIFICATION - **PASS**

All planned breaking changes were successfully implemented:

| Breaking Change | Status | Evidence |
|----------------|--------|----------|
| Old `loader.py` deleted | âœ… PASS | Only `loader.py.old.bak` remains (backup) |
| `load_llm()` removed | âœ… PASS | Not exported in `llms/__init__.py` |
| `SupportsStream` removed | âœ… PASS | Only in backup files |
| `ModelName` enum removed | âœ… PASS | Only in backups + 1 README.md ref (needs fix) |
| `validate_lrm()` removed | âœ… PASS | Replaced with `validate_agent()` |
| GGUF support removed | âœ… PASS | No `gguf_strategy()` references found |

#### âœ… CODE QUALITY - **EXCELLENT**

**agent_loader.py Assessment:**
- **Security**: âœ… Implements `sanitize_log_str()` to prevent log injection attacks
- **Error Handling**: âœ… Comprehensive try/except blocks with meaningful messages
- **Async Patterns**: âœ… Proper async/await usage throughout
- **Logging**: âœ… Uses `midori_ai_logger` consistently
- **Architecture**: âœ… Clean separation: config loading â†’ backend detection â†’ agent creation
- **Fallback Logic**: âœ… Graceful degradation: Config file â†’ Env vars â†’ Defaults
- **Documentation**: âœ… Clear docstrings with type hints

**Production Code Migration Quality:**
- `app.py`: âœ… Clean migration, proper async usage
- `routes/config.py`: âœ… Uses new interfaces correctly
- `autofighter/rooms/chat.py`: âœ… Proper AgentPayload usage
- `plugins/characters/_base.py`: âœ… Good fallback handling
- `plugins/characters/foe_base.py`: âœ… Consistent with _base.py

**safety.py Status:**
- âœ… GGUF support properly removed (no `gguf_strategy()` found)
- âœ… Kept functions work with HuggingFace model names
- âœ… No breaking issues identified

#### âš ï¸ TEST COVERAGE - **INCOMPLETE**

**Passing Tests:**
- âœ… `test_agent_loader.py` - All 7 tests pass (100% coverage of new loader)
  - Framework availability checks âœ“
  - Backend auto-detection âœ“
  - Validation logic âœ“

**Critical Issues:**
- âŒ **`test_llm_loader.py` is EMPTY (0 bytes)** - Must be fixed
- âŒ `test_accelerate_dependency.py` - 2 failures (import structure issues)
- âŒ `test_chat_room.py` - 1 failure (ChatRoom init - may be unrelated)
- âŒ `test_config_lrm.py` - 2 failures (1 monkeypatch issue, 1 unrelated)

**Test Failure Analysis:**

1. **test_accelerate_dependency.py** (2 failures):
   ```
   TypeError: object NoneType can't be used in 'await' expression
   ```
   **Root Cause**: Tests import `load_agent` AFTER patching, causing stale reference
   **Impact**: HIGH - These tests validate agent framework availability
   **Fix Required**: Refactor to patch before import OR use reload()

2. **test_chat_room.py** (1 failure):
   ```
   TypeError: ChatRoom.__init__() missing 1 required positional argument: 'node'
   ```
   **Root Cause**: Test instantiates ChatRoom() without required 'node' parameter
   **Impact**: MEDIUM - Likely pre-existing issue, not migration-related
   **Recommendation**: Fix in separate task or update test

3. **test_config_lrm.py** (2 failures):
   - KeyError: 'response' - Monkeypatch not applying correctly
   - Pacing value mismatch - Unrelated to migration
   **Impact**: MEDIUM - LRM endpoint testing affected
   **Fix Required**: Review monkeypatch setup

#### âŒ DOCUMENTATION - **INCOMPLETE**

**Issues Found:**
1. âŒ **README.md line 97** still references `ModelName` enum:
   ```
   `GET /config/lrm` returns the current model and available `ModelName` values.
   ```
   **Impact**: HIGH - Misleading for users
   **Fix**: Update to reference string model names instead

2. âš ï¸ **Migration guide not created** (mentioned in execution plan Phase 8)
   **Impact**: MEDIUM - Users may struggle with migration
   **Recommendation**: Add to `.codex/implementation/`

3. âš ï¸ **Backup files retained**:
   - `backend/llms/loader.py.old.bak`
   - `backend/tests/test_llm_loader.py.old.bak`
   **Impact**: LOW - Cleanup recommended
   **Recommendation**: Document retention policy or remove

#### âœ… LINTING - **PASS**

**Status**: âœ… Only pre-existing issues found
- Minor E402 errors in `test_async_improvements.py` (module-level import placement)
- **Not related to migration** - pre-existing technical debt
- Backend passes linting for migration-related files

#### âœ… CODE CONSISTENCY - **PASS**

**Repository Standards Compliance:**
- âœ… Async patterns properly implemented
- âœ… Import ordering follows style guide (mostly)
- âœ… Error handling consistent with codebase
- âœ… Logging uses approved framework (midori_ai_logger)
- âœ… Type hints present and accurate

### ðŸš¨ REQUIRED FIXES (BLOCKING)

These issues **MUST** be resolved before validation:

#### 1. âŒ test_llm_loader.py is EMPTY (0 bytes)
**Priority**: CRITICAL  
**Action Required**: Choose one:
- **Option A** (Recommended): Delete the file entirely
  ```bash
  rm backend/tests/test_llm_loader.py
  ```
- **Option B**: Populate with backward compatibility tests (if needed)

**Rationale**: Empty test files break test discovery and indicate incomplete work

#### 2. âŒ Update README.md line 97
**Priority**: HIGH  
**Current Text**:
```
`GET /config/lrm` returns the current model and available `ModelName` values.
```

**Required Change**:
```
`GET /config/lrm` returns the current model and available model string values.
```

#### 3. âŒ Fix test_accelerate_dependency.py
**Priority**: HIGH  
**Issue**: Import happens after patching, causing stale reference  
**Required**: Refactor to use `importlib.reload()` or restructure tests

### ðŸ“‹ RECOMMENDED FIXES (NON-BLOCKING)

These should be addressed but don't block validation:

#### 4. âš ï¸ Create Migration Guide
**Priority**: MEDIUM  
**Location**: `.codex/implementation/llm-migration-guide.md`  
**Content**: Document old â†’ new API patterns, breaking changes, examples

#### 5. âš ï¸ Clean Up Backup Files
**Priority**: LOW  
**Action**: Either document retention policy or remove:
- `backend/llms/loader.py.old.bak`
- `backend/tests/test_llm_loader.py.old.bak`

#### 6. âš ï¸ Fix test_chat_room.py
**Priority**: MEDIUM  
**Note**: May be unrelated to migration - investigate ChatRoom API changes

#### 7. âš ï¸ Fix test_config_lrm.py
**Priority**: MEDIUM  
**Note**: Monkeypatch setup needs review

### âœ… ACCEPTANCE CRITERIA STATUS

| Criterion | Status | Notes |
|-----------|--------|-------|
| New `agent_loader.py` created | âœ… PASS | Excellent implementation |
| NO compatibility layer | âœ… PASS | Breaking changes intentional |
| `llms/__init__.py` exports only new interfaces | âœ… PASS | Clean exports |
| Old `loader.py` DELETED | âœ… PASS | Backup retained |
| Test script validates new loader | âœ… PASS | `test_agent_loader.py` comprehensive |
| `ModelName` enum REMOVED | âš ï¸ PARTIAL | Removed from code, 1 doc ref remains |
| GGUF support REMOVED | âœ… PASS | Fully removed |
| `torch_checker.py` kept | âœ… PASS | Functional |
| `safety.py` works with agents | âœ… PASS | Verified |
| Linting passes | âœ… PASS | Migration code clean |
| Breaking changes documented | âš ï¸ PARTIAL | In code, not in migration guide |

### ðŸŽ¯ NEXT STEPS FOR VALIDATION

1. **Coder Actions Required:**
   - [ ] Fix CRITICAL: Delete or populate `test_llm_loader.py`
   - [ ] Fix HIGH: Update README.md line 97
   - [ ] Fix HIGH: Refactor `test_accelerate_dependency.py`
   - [ ] Optional: Create migration guide
   - [ ] Optional: Clean up .old.bak files

2. **After Fixes:**
   - [ ] Run full test suite: `./run-tests.sh`
   - [ ] Verify linting: `uv tool run ruff check backend --fix`
   - [ ] Manual smoke test: Start app, test LRM endpoints
   - [ ] Build verification: `./build.sh non-llm`

3. **Task Movement:**
   - Once all CRITICAL and HIGH issues resolved â†’ Move to `.codex/tasks/review/`
   - After validation passes â†’ Move to `.codex/tasks/taskmaster/`

### ðŸ“Š AUDIT SCORE SUMMARY

| Category | Score | Status |
|----------|-------|--------|
| Breaking Changes | 100% | âœ… PASS |
| Code Quality | 95% | âœ… EXCELLENT |
| Test Coverage | 60% | âš ï¸ INCOMPLETE |
| Documentation | 70% | âš ï¸ INCOMPLETE |
| Linting | 100% | âœ… PASS |
| Standards Compliance | 95% | âœ… PASS |
| **OVERALL** | **85%** | âš ï¸ CONDITIONAL PASS |

**Verdict**: Migration is functionally complete and well-implemented, but requires cleanup of critical issues before final validation. Core functionality is solid and ready for use once tests are fixed.

---

**Auditor**: GitHub Copilot Auditor Mode  
**Date**: 2025-12-20  
**Compliance**: All 7 Auditor Mode directives followed

## Description
Replace the custom `backend/llms/loader.py` module with the Midori AI Agent Framework's standardized agent interface. This eliminates custom wrappers and protocols in favor of the framework's `MidoriAiAgentProtocol`, `AgentPayload`, and `AgentResponse`.

## Context
Currently, `backend/llms/loader.py` implements:
- Custom `SupportsStream` protocol
- `_LangChainWrapper` for HuggingFace models
- `_OpenAIAgentsWrapper` for remote OpenAI APIs
- Manual model loading logic
- Custom validation functions
- `load_llm()` function that returns wrapper objects

The Agent Framework provides:
- `MidoriAiAgentProtocol` standard interface
- Pre-built adapters (OpenAIAgentsAdapter, HuggingFaceLocalAgent, LangChainAdapter)
- `get_agent()` factory function
- `AgentPayload` and `AgentResponse` standard models

## Rationale
- **Less Code**: Remove ~250 lines of custom wrapper code
- **Standardization**: Use framework's proven interfaces
- **Better Features**: Get streaming, tools, memory support built-in
- **Easier Testing**: Framework includes test utilities
- **Future-Proof**: New backends can be added via framework updates
- **Breaking Changes**: Intentionally break old code to find issues faster

## Objectives
1. Replace custom wrappers with framework adapters
2. Replace `load_llm()` with `load_agent()` using `get_agent()` factory
3. Replace `SupportsStream` protocol with `MidoriAiAgentProtocol`
4. **BREAK backward compatibility** - remove all old code and compatibility layers
5. Remove GGUF support (not needed with agent framework)
6. Keep torch_checker.py for system checks, verify safety.py works with agents
7. Focus on OpenAI agents (primary) and HuggingFace agents (fallback)

## Implementation Steps

### Step 1: Understand Current Usage
Review how `load_llm()` is currently used:
```bash
cd backend
grep -rn "from llms.loader import" . --include="*.py" | grep -v ".venv"
grep -rn "load_llm(" . --include="*.py" | grep -v ".venv"
```

Document all call sites:
- `autofighter/rooms/chat.py` - Chat room
- `routes/config.py` - LRM testing endpoint
- Any other locations

### Step 2: Create New Agent Loader
Create `backend/llms/agent_loader.py`:

```python
"""Agent loader using Midori AI Agent Framework."""
from __future__ import annotations

import os
from typing import TYPE_CHECKING

from midori_ai_agent_base import MidoriAiAgentProtocol, get_agent
from midori_ai_logger import get_logger

if TYPE_CHECKING:
    from midori_ai_agent_base import AgentPayload, AgentResponse

from .torch_checker import is_torch_available

log = get_logger(__name__)


async def load_agent(
    backend: str | None = None,
    model: str | None = None,
    validate: bool = True,
) -> MidoriAiAgentProtocol:
    """Load an agent using the Midori AI Agent Framework.
    
    Primary backend: OpenAI agents (for OpenAI API, Ollama, LocalAI, etc.)
    Fallback backend: HuggingFace agents (for local inference)
    
    Args:
        backend: Backend type ("openai" or "huggingface")
                 If None, auto-selects: openai if URL set, else huggingface if torch available
        model: Model name to use
               If None, uses environment variable or default
        validate: Whether to validate the agent (future use)
    
    Returns:
        Agent implementing MidoriAiAgentProtocol
    
    Raises:
        ImportError: If required libraries are not available
        ValueError: If configuration is invalid
    """
    # Auto-detect backend: prioritize OpenAI, fallback to HuggingFace
    if backend is None:
        openai_url = os.getenv("OPENAI_API_URL", "unset")
        if openai_url != "unset":
            backend = "openai"
        elif is_torch_available():
            backend = "huggingface"
            # Use recommended model for HuggingFace with high reasoning
            if model is None:
                model = "openai/gpt-oss-20b"
        else:
            raise ValueError("No backend available. Set OPENAI_API_URL or install torch for local inference.")
    
    # Get model from environment if not specified
    if model is None:
        model = os.getenv("AF_LLM_MODEL", "gpt-oss:20b")
    
    # Get API configuration (for OpenAI backend)
    api_key = os.getenv("OPENAI_API_KEY", "")
    base_url = os.getenv("OPENAI_API_URL")
    
    # Create agent using framework factory
    agent = await get_agent(
        backend=backend,
        model=model,
        api_key=api_key,
        base_url=base_url,
    )
    
    if validate:
        log.info(f"Agent loaded: backend={backend}, model={model}")
    
    return agent


async def validate_agent(agent: MidoriAiAgentProtocol) -> bool:
    """Validate that an agent is working correctly.
    
    Args:
        agent: The agent to validate
    
    Returns:
        True if validation passes, False otherwise
    """
    from midori_ai_agent_base import AgentPayload
    
    try:
        test_payload = AgentPayload(
            user_message="Please say 'Hello world' and 5 words about yourself",
            thinking_blob="",
            system_context="You are a helpful assistant.",
            user_profile={},
            tools_available=[],
            session_id="validation",
        )
        
        response = await agent.invoke(test_payload)
        
        # Check if response is longer than 5 characters
        if len(response.response) > 5:
            log.info(f"Agent validation passed. Response length: {len(response.response)}")
            return True
        else:
            log.warning(f"Agent validation failed. Response too short: {response.response}")
            return False
    except Exception as e:
        log.error(f"Agent validation failed with error: {e}")
        return False


__all__ = ["load_agent", "validate_agent"]
```

### Step 3: Update llms/__init__.py (NO Compatibility Layer)
Update `backend/llms/__init__.py` to export ONLY new interfaces:

```python
"""LLM and agent loading utilities - NEW AGENT FRAMEWORK ONLY."""

# New agent framework interface (ONLY interface now)
from .agent_loader import load_agent
from .agent_loader import validate_agent

# Keep existing utilities
from .torch_checker import is_torch_available
from .torch_checker import require_torch

__all__ = [
    # New interface (breaking change - old code must update)
    "load_agent",
    "validate_agent",
    # Utilities
    "is_torch_available",
    "require_torch",
]

# OLD INTERFACES REMOVED:
# - load_llm() - REMOVED, use load_agent() instead
# - SupportsStream - REMOVED, use MidoriAiAgentProtocol instead
```

### Step 4: Remove Old loader.py
Delete or archive the old loader:
```bash
cd backend/llms
# Archive old loader for reference
mv loader.py loader.py.old.bak
# Or delete it entirely
# rm loader.py
```
    # Run async load_agent in sync context
    agent = asyncio.run(_load_agent(model=model, validate=validate))
    return _AgentStreamWrapper(agent)


__all__ = ["load_llm", "SupportsStream"]
```

### Step 4: Update llms/__init__.py
Update `backend/llms/__init__.py`:

```python
"""LLM and agent loading utilities."""

# New agent framework interface (preferred)
from .agent_loader import load_agent
from .agent_loader import validate_agent

# Legacy compatibility interface
from .compat import SupportsStream
from .compat import load_llm

# Keep existing utilities
from .torch_checker import is_torch_available
from .torch_checker import require_torch

__all__ = [
    # New interface
    "load_agent",
    "validate_agent",
    # Legacy interface
    "load_llm",
    "SupportsStream",
    # Utilities
    "is_torch_available",
    "require_torch",
]
```

### Step 5: Remove ModelName Enum (Breaking Change)
The old `ModelName` enum is removed. Update all code to use model strings directly:
- Old: `ModelName.OPENAI_20B.value`
- New: `"openai/gpt-oss-20b"`

**GGUF support is REMOVED** - not needed with agent framework.

Find and update all references:
```bash
cd backend
grep -rn "ModelName" . --include="*.py" | grep -v ".venv"
```

### Step 6: Verify safety.py Works with Agent Framework
Review `backend/llms/safety.py` to ensure functions work with Midori AI HuggingFace agents:
```bash
cd backend/llms
cat safety.py
```

Key functions to verify:
- `model_memory_requirements()` - Should work with HuggingFace model names
- `pick_device()` - Should work with agent framework device selection
- `gguf_strategy()` - **REMOVE** (GGUF support dropped)

Update or remove incompatible functions.

### Step 7: Test the New Loader
Create test script `backend/test_agent_loader.py`:

```python
"""Test the new agent loader."""
import asyncio
from midori_ai_agent_base import AgentPayload
from midori_ai_logger import get_logger
from llms.agent_loader import load_agent, validate_agent

log = get_logger(__name__)


async def test_agent_loader():
    """Test loading and using an agent."""
    # Test with HuggingFace backend (local, no external dependencies)
    log.info("Loading agent with HuggingFace backend...")
    agent = await load_agent(backend="huggingface", model="openai/gpt-oss-20b")
    
    log.info("Validating agent...")
    is_valid = await validate_agent(agent)
    log.info(f"Validation: {'PASSED' if is_valid else 'FAILED'}")
    
    log.info("Testing agent invoke...")
    payload = AgentPayload(
        user_message="What is 2+2?",
        thinking_blob="",
        system_context="You are a math tutor.",
        user_profile={},
        tools_available=[],
        session_id="test",
    )
    
    response = await agent.invoke(payload)
    log.info(f"Response: {response.response}")
    
    log.info("Testing streaming...")
    if await agent.supports_streaming():
        log.info("Streaming supported, testing...")
        async for chunk in agent.stream(payload):
            print(chunk, end="", flush=True)
        print()
    else:
        log.info("Streaming not supported")


if __name__ == "__main__":
    asyncio.run(test_agent_loader())
```

Run test:
```bash
cd backend
uv run python test_agent_loader.py
```

### Step 8: Clean Up Old Code (Breaking Change)
Delete old code completely:
```bash
cd backend/llms
# DELETE old loader
rm loader.py

# No compat.py needed - breaking change is intentional
```

All code using `load_llm()` must be updated to use `load_agent()`.

## Acceptance Criteria
- [ ] New `agent_loader.py` created with `load_agent()` function using midori_ai_logger
- [ ] **NO compatibility layer** - breaking changes intentional
- [ ] `llms/__init__.py` updated to export ONLY new interfaces
- [ ] Old `loader.py` DELETED
- [ ] Test script validates new loader works with HuggingFace backend
- [ ] `ModelName` enum REMOVED completely
- [ ] **GGUF support REMOVED**
- [ ] `torch_checker.py` kept and functional
- [ ] `safety.py` verified to work with agents or updated
- [ ] All old `load_llm()` calls must be manually updated (breaking change)
- [ ] Linting passes (`uv tool run ruff check backend --fix`)
- [ ] Intentional breaking changes documented

## Testing Requirements

### Unit Tests
Create `backend/tests/test_agent_loader.py`:
```python
import pytest
from llms.agent_loader import load_agent, validate_agent
from midori_ai_agent_base import AgentPayload


@pytest.mark.asyncio
async def test_load_agent():
    """Test agent loading."""
    agent = await load_agent(backend="huggingface", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")
    assert agent is not None


@pytest.mark.asyncio
async def test_validate_agent():
    """Test agent validation."""
    agent = await load_agent(backend="huggingface", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")
    is_valid = await validate_agent(agent)
    assert is_valid is True


@pytest.mark.asyncio
async def test_agent_invoke():
    """Test agent invocation."""
    agent = await load_agent(backend="huggingface", model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")
    payload = AgentPayload(
        user_message="Hello",
        thinking_blob="",
        system_context="You are helpful.",
        user_profile={},
        tools_available=[],
        session_id="test",
    )
    response = await agent.invoke(payload)
    assert len(response.response) > 0
```

### Integration Tests
```bash
# Test with actual models
cd backend
uv run pytest tests/test_agent_loader.py -v
```

## Dependencies
- Requires: `12af34e9-update-dependencies.md` (must be completed first)
- Blocks: `c0f04e25-update-chat-room.md`
- Blocks: `96e5fdb9-update-config-routes.md`
- Blocks: `5e609bc6-webui-backend-selection.md` (new WebUI task)
- Related: `656b2a7e-create-config-support.md` (can be done in parallel)

## References
- [midori-ai-agent-base docs](https://github.com/Midori-AI-OSS/agents-packages/blob/main/midori-ai-agent-base/docs.md)
- [midori-ai-agent-openai docs](https://github.com/Midori-AI-OSS/agents-packages/blob/main/midori-ai-agent-openai/docs.md)
- [midori-ai-agent-huggingface docs](https://github.com/Midori-AI-OSS/agents-packages/blob/main/midori-ai-agent-huggingface/docs.md)
- [midori-ai-logger](https://github.com/Midori-AI-OSS/agents-packages/tree/main/logger)
- Current: `backend/llms/loader.py` (to be deleted)
- Current: `backend/llms/torch_checker.py` (keep)
- Current: `backend/llms/safety.py` (verify/update)

## Notes for Coder
- **BREAK backward compatibility** - this is intentional to find issues faster
- NO compatibility layer - all old code must be updated
- The framework's `get_agent()` handles backend selection automatically
- AgentPayload is more structured than the old text-only interface
- Remove ModelName enum completely - use strings directly
- Remove GGUF support - not needed with agent framework
- Use midori_ai_logger for all logging (replace `logging.getLogger()`)
- Primary: OpenAI agents (for API servers), Fallback: HuggingFace agents (local)
- The framework includes memory management - don't need custom implementation
- Streaming is built into the framework - simpler than custom implementation
- Verify safety.py functions work with agent framework model names
